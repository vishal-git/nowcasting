{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f4c3054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Marketing Cost Prediction - Gradient Boosting Model ===\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "   Unnamed: 0     aov_eur  available_stock_value_after_discount_complete_eur  \\\n",
      "0  2022-03-14  144.754190                                       4.834682e+07   \n",
      "1  2022-03-21  149.213739                                       4.980875e+07   \n",
      "2  2022-03-28  142.841626                                       5.277616e+07   \n",
      "3  2022-04-04  151.042129                                       5.601095e+07   \n",
      "4  2022-04-11  153.828542                                       5.911150e+07   \n",
      "\n",
      "   avg_temp       cpc  cr_tracked_%  email_recipients  email_visits  \\\n",
      "0  4.771429  0.369265      0.030110          754257.0       23604.0   \n",
      "1  6.742857  0.397933      0.030137          966739.0       43888.0   \n",
      "2  8.657143  0.391481      0.029470          580566.0       12686.0   \n",
      "3  3.657143  0.404580      0.030292         1118797.0       38267.0   \n",
      "4  6.900000  0.397859      0.026801          734504.0       20674.0   \n",
      "\n",
      "   internalWeeks_until_SeasonalSaleStart  internal_Week_of_FW_Season  ...  \\\n",
      "0                              19.857143                         0.0  ...   \n",
      "1                              18.857143                         0.0  ...   \n",
      "2                              17.857143                         0.0  ...   \n",
      "3                              16.857143                         0.0  ...   \n",
      "4                              15.857143                         0.0  ...   \n",
      "\n",
      "   is_season_sale_event  is_temp_drop_flag  number_days_after_last_event  \\\n",
      "0                   0.0                0.0                     12.000000   \n",
      "1                   3.0                0.0                      8.000000   \n",
      "2                   0.0                0.0                      6.000000   \n",
      "3                   0.0                0.0                      5.428571   \n",
      "4                   0.0                0.0                      6.000000   \n",
      "\n",
      "   number_days_till_next_event  number_orders  number_visits  \\\n",
      "0                     7.000000        32355.0       793554.0   \n",
      "1                     2.428571        35243.0       859932.0   \n",
      "2                     7.000000        29804.0       756419.0   \n",
      "3                    14.714286        34443.0       828831.0   \n",
      "4                    93.000000        28298.0       777534.0   \n",
      "\n",
      "   sku_with_discount_%  stock_discount_rate_total_%  target_cpr  \\\n",
      "0             0.894808                     0.233624    0.060214   \n",
      "1             0.863678                     0.219791    0.061314   \n",
      "2             0.802163                     0.202488    0.062214   \n",
      "3             0.688411                     0.182454    0.062557   \n",
      "4             0.755250                     0.175509    0.062043   \n",
      "\n",
      "   marketing_cost  \n",
      "0     264849.2007  \n",
      "1     299520.3995  \n",
      "2     268467.7618  \n",
      "3     303469.6580  \n",
      "4     282268.2929  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "# Marketing Cost Prediction - Gradient Boosting Model\n",
    "# This notebook implements a comprehensive gradient boosting model with feature engineering,\n",
    "# proper scaling, hyperparameter tuning, and model interpretability\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score, \n",
    "    mean_absolute_percentage_error, explained_variance_score\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(314)\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = 'Data_for_taining_14072025.csv'\n",
    "TARGET = 'marketing_cost'\n",
    "RANDOM_STATE = 314\n",
    "\n",
    "print(\"=== Marketing Cost Prediction - Gradient Boosting Model ===\\n\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Display first few rows of the dataset\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65187182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading and exploring data...\n",
      "Dataset shape: (171, 28)\n",
      "Date range: 2022-03-21 00:00:00 to 2025-06-23 00:00:00\n",
      "Number of features: 27\n",
      "Target variable: marketing_cost\n",
      "\n",
      "Target variable statistics:\n",
      "count    1.710000e+02\n",
      "mean     4.228278e+05\n",
      "std      1.456366e+05\n",
      "min      2.241357e+05\n",
      "25%      3.334185e+05\n",
      "50%      3.861772e+05\n",
      "75%      4.873196e+05\n",
      "max      1.251066e+06\n",
      "Name: marketing_cost, dtype: float64\n",
      "\n",
      "Missing values per column:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# 1. Data Loading and Initial Exploration\n",
    "print(\"1. Loading and exploring data...\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.rename(columns={'Unnamed: 0': 'date'}, inplace=True)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.sort_values('date', inplace=True)\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Remove first and last week as in original approach\n",
    "df = df.iloc[1:-1]\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "print(f\"Number of features: {len(df.columns) - 1}\")  # Excluding target\n",
    "print(f\"Target variable: {TARGET}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nTarget variable statistics:\")\n",
    "print(df[TARGET].describe())\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da5c1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Feature Engineering...\n",
      "Creating lag features...\n",
      "Creating rolling features...\n",
      "Creating interaction features...\n",
      "Shape after feature engineering: (171, 38)\n",
      "New features created: 10\n"
     ]
    }
   ],
   "source": [
    "# 2. Feature Engineering\n",
    "print(\"\\n2. Feature Engineering...\")\n",
    "\n",
    "def create_lag_features(df, target_col, lags=[1, 2, 4]):\n",
    "    \"\"\"Create lag features for time series data\"\"\"\n",
    "    df_lagged = df.copy()\n",
    "    for lag in lags:\n",
    "        df_lagged[f'{target_col}_lag_{lag}'] = df_lagged[target_col].shift(lag)\n",
    "    return df_lagged\n",
    "\n",
    "def create_rolling_features(df, target_col, windows=[2, 4]):\n",
    "    \"\"\"Create rolling window features\"\"\"\n",
    "    df_rolling = df.copy()\n",
    "    for window in windows:\n",
    "        df_rolling[f'{target_col}_rolling_mean_{window}'] = df_rolling[target_col].rolling(window=window).mean()\n",
    "        df_rolling[f'{target_col}_rolling_std_{window}'] = df_rolling[target_col].rolling(window=window).std()\n",
    "    return df_rolling\n",
    "\n",
    "def create_interaction_features(df):\n",
    "    \"\"\"Create interaction features between key variables\"\"\"\n",
    "    df_interactions = df.copy()\n",
    "    \n",
    "    # Revenue-related interactions\n",
    "    df_interactions['revenue_per_visit'] = df_interactions['number_orders'] * df_interactions['aov_eur'] / df_interactions['number_visits']\n",
    "    df_interactions['conversion_rate'] = df_interactions['number_orders'] / df_interactions['number_visits']\n",
    "    \n",
    "    # Marketing efficiency\n",
    "    df_interactions['roas'] = (df_interactions['number_orders'] * df_interactions['aov_eur']) / df_interactions['marketing_cost']\n",
    "    \n",
    "    return df_interactions\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"Creating lag features...\")\n",
    "df_engineered = create_lag_features(df, TARGET, lags=[1, 2, 4])\n",
    "\n",
    "print(\"Creating rolling features...\")\n",
    "df_engineered = create_rolling_features(df_engineered, TARGET, windows=[2, 4])\n",
    "\n",
    "print(\"Creating interaction features...\")\n",
    "df_engineered = create_interaction_features(df_engineered)\n",
    "\n",
    "# Handle infinite values from division\n",
    "df_engineered = df_engineered.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "print(f\"Shape after feature engineering: {df_engineered.shape}\")\n",
    "print(f\"New features created: {df_engineered.shape[1] - df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35463e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Data Preprocessing...\n",
      "Before dropping missing values: 171 rows\n",
      "After dropping missing values: 167 rows\n",
      "Final dataset shape: (167, 37)\n",
      "Feature columns: ['aov_eur', 'available_stock_value_after_discount_complete_eur', 'avg_temp', 'cpc', 'cr_tracked_%', 'email_recipients', 'email_visits', 'internalWeeks_until_SeasonalSaleStart', 'internal_Week_of_FW_Season', 'internal_Week_of_SS_Season', 'is_Peak_Driving_Public_Holiday_week', 'is_Sun_to_Mon_Shift_week', 'is_black_week_event', 'is_email_campaign_type_deal', 'is_email_campaign_type_liveshop', 'is_email_campaign_type_newsletter', 'is_percentage_on_top', 'is_percentage_on_top_applicable', 'is_season_sale_event', 'is_temp_drop_flag', 'number_days_after_last_event', 'number_days_till_next_event', 'number_orders', 'number_visits', 'sku_with_discount_%', 'stock_discount_rate_total_%', 'target_cpr', 'marketing_cost_lag_1', 'marketing_cost_lag_2', 'marketing_cost_lag_4', 'marketing_cost_rolling_mean_2', 'marketing_cost_rolling_std_2', 'marketing_cost_rolling_mean_4', 'marketing_cost_rolling_std_4', 'revenue_per_visit', 'conversion_rate', 'roas']\n",
      "Training set: 133 samples\n",
      "Test set: 34 samples\n"
     ]
    }
   ],
   "source": [
    "# 3. Data Preprocessing\n",
    "print(\"\\n3. Data Preprocessing...\")\n",
    "\n",
    "# Drop rows with missing values (as per business requirement)\n",
    "print(f\"Before dropping missing values: {df_engineered.shape[0]} rows\")\n",
    "df_clean = df_engineered.dropna()\n",
    "print(f\"After dropping missing values: {df_clean.shape[0]} rows\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df_clean.drop(columns=[TARGET])\n",
    "y = df_clean[TARGET]\n",
    "\n",
    "print(f\"Final dataset shape: {X.shape}\")\n",
    "print(f\"Feature columns: {list(X.columns)}\")\n",
    "\n",
    "# Split data (keeping temporal order)\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train = X.iloc[:train_size]\n",
    "X_test = X.iloc[train_size:]\n",
    "y_train = y.iloc[:train_size]\n",
    "y_test = y.iloc[train_size:]\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "367ea7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Feature Scaling...\n",
      "Feature scaling completed\n",
      "Training data scaled shape: (133, 37)\n",
      "Test data scaled shape: (34, 37)\n",
      "\n",
      "Scaling effect on sample features:\n",
      "aov_eur:\n",
      "  Original - Mean: 157.94, Std: 12.97\n",
      "  Scaled - Mean: 0.00, Std: 1.00\n",
      "available_stock_value_after_discount_complete_eur:\n",
      "  Original - Mean: 81141833.61, Std: 17128045.95\n",
      "  Scaled - Mean: 0.00, Std: 1.00\n",
      "avg_temp:\n",
      "  Original - Mean: 11.93, Std: 6.41\n",
      "  Scaled - Mean: 0.00, Std: 1.00\n"
     ]
    }
   ],
   "source": [
    "# 4. Feature Scaling\n",
    "print(\"\\n4. Feature Scaling...\")\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler on training data only\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"Feature scaling completed\")\n",
    "print(f\"Training data scaled shape: {X_train_scaled_df.shape}\")\n",
    "print(f\"Test data scaled shape: {X_test_scaled_df.shape}\")\n",
    "\n",
    "# Show scaling effect on a few features\n",
    "print(\"\\nScaling effect on sample features:\")\n",
    "sample_features = ['aov_eur', 'available_stock_value_after_discount_complete_eur', 'avg_temp']\n",
    "for feature in sample_features:\n",
    "    if feature in X_train.columns:\n",
    "        print(f\"{feature}:\")\n",
    "        print(f\"  Original - Mean: {X_train[feature].mean():.2f}, Std: {X_train[feature].std():.2f}\")\n",
    "        print(f\"  Scaled - Mean: {X_train_scaled_df[feature].mean():.2f}, Std: {X_train_scaled_df[feature].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d42e4db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Hyperparameter Tuning...\n",
      "Performing grid search...\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "Best parameters: {'learning_rate': 0.2, 'max_depth': 6, 'min_samples_split': 0.03, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Best cross-validation score: 1970664557.25\n"
     ]
    }
   ],
   "source": [
    "# 5. Hyperparameter Tuning\n",
    "print(\"\\n5. Hyperparameter Tuning...\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 6, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'min_samples_split': [0.03, 0.05, 0.04],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize base model\n",
    "base_model = GradientBoostingRegressor(random_state=RANDOM_STATE)\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Performing grid search...\")\n",
    "grid_search = GridSearchCV(\n",
    "    base_model,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {-grid_search.best_score_:.2f}\")\n",
    "\n",
    "# Get best model\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40957a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Model Training and Evaluation...\n",
      "\n",
      "Training Metrics:\n",
      "MAE: 0.00\n",
      "RMSE: 0.00\n",
      "R²: 1.0000\n",
      "MAPE: 0.0000\n",
      "Explained Variance: 1.0000\n",
      "\n",
      "Test Metrics:\n",
      "MAE: 25478.07\n",
      "RMSE: 32053.35\n",
      "R²: 0.9644\n",
      "MAPE: 0.0487\n",
      "Explained Variance: 0.9779\n"
     ]
    }
   ],
   "source": [
    "# 6. Model Training and Evaluation\n",
    "print(\"\\n6. Model Training and Evaluation...\")\n",
    "\n",
    "# Train the best model\n",
    "best_model.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = best_model.predict(X_train_scaled_df)\n",
    "y_test_pred = best_model.predict(X_test_scaled_df)\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred, set_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    evs = explained_variance_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{set_name} Metrics:\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"MAPE: {mape:.4f}\")\n",
    "    print(f\"Explained Variance: {evs:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'mape': mape,\n",
    "        'explained_variance': evs\n",
    "    }\n",
    "\n",
    "# Calculate metrics for both sets\n",
    "train_metrics = calculate_metrics(y_train, y_train_pred, \"Training\")\n",
    "test_metrics = calculate_metrics(y_test, y_test_pred, \"Test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
